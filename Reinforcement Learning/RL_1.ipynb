{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bfc7ebd",
   "metadata": {},
   "source": [
    "#### Question 1: Train a CartPole Agent\n",
    "#### Dataset Problem: Use the OpenAI Gym's CartPole-v1 environment to train an agent using a simple reinforcement learning algorithm.  Assume hyperparameters, as per requirement and develop the model. Try to apply the concepts discussed in class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b96bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "     -------------- ------------------------- 262.1/721.7 kB ? eta -:--:--\n",
      "     -------------------------- --------- 524.3/721.7 kB 670.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 721.7/721.7 kB 890.6 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gym) (1.24.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Collecting gym-notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827633 sha256=2e2f2233f8bbe14cc75c37ce4295e26e78982a7b0f1ed87a2735cadbdd8da3cb\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: gym-notices, gym\n",
      "Successfully installed gym-0.26.2 gym-notices-0.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\USER\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\USER\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\USER\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gym torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26d91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define the neural network architecture for the Q-network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Set up hyperparameters\n",
    "gamma = 0.99          # Discount factor for future rewards\n",
    "epsilon = 1.0         # Exploration-exploitation trade-off factor\n",
    "epsilon_min = 0.01    # Minimum value for epsilon\n",
    "epsilon_decay = 0.995 # Decay rate of epsilon per episode\n",
    "learning_rate = 0.001 # Learning rate for Q-network\n",
    "batch_size = 64       # Batch size for experience replay\n",
    "memory_size = 10000   # Size of experience replay buffer\n",
    "num_episodes = 500    # Total number of episodes for training\n",
    "\n",
    "# Initialize environment and agent\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialize DQN and target networks\n",
    "q_network = DQN(state_dim, action_dim)\n",
    "target_network = DQN(state_dim, action_dim)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "# Set up optimizer and replay memory\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)\n",
    "replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "# Function to select action based on epsilon-greedy policy\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()  # Random action\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            return int(torch.argmax(q_values).item())\n",
    "\n",
    "# Function to train the Q-network\n",
    "def train_q_network():\n",
    "    if len(replay_memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Sample a mini-batch from the replay memory\n",
    "    minibatch = random.sample(replay_memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "\n",
    "    # Compute the current Q values\n",
    "    q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Compute the target Q values using the target network\n",
    "    with torch.no_grad():\n",
    "        max_next_q_values = target_network(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * gamma * max_next_q_values\n",
    "\n",
    "    # Calculate loss and optimize the Q-network\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8082f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: 15.0, Epsilon: 0.99\n",
      "Episode 2/500, Total Reward: 14.0, Epsilon: 0.99\n",
      "Episode 3/500, Total Reward: 17.0, Epsilon: 0.99\n",
      "Episode 4/500, Total Reward: 23.0, Epsilon: 0.98\n",
      "Episode 5/500, Total Reward: 10.0, Epsilon: 0.98\n",
      "Episode 6/500, Total Reward: 15.0, Epsilon: 0.97\n",
      "Episode 7/500, Total Reward: 11.0, Epsilon: 0.97\n",
      "Episode 8/500, Total Reward: 36.0, Epsilon: 0.96\n",
      "Episode 9/500, Total Reward: 13.0, Epsilon: 0.96\n",
      "Episode 10/500, Total Reward: 54.0, Epsilon: 0.95\n",
      "Episode 11/500, Total Reward: 15.0, Epsilon: 0.95\n",
      "Episode 12/500, Total Reward: 11.0, Epsilon: 0.94\n",
      "Episode 13/500, Total Reward: 12.0, Epsilon: 0.94\n",
      "Episode 14/500, Total Reward: 22.0, Epsilon: 0.93\n",
      "Episode 15/500, Total Reward: 31.0, Epsilon: 0.93\n",
      "Episode 16/500, Total Reward: 24.0, Epsilon: 0.92\n",
      "Episode 17/500, Total Reward: 32.0, Epsilon: 0.92\n",
      "Episode 18/500, Total Reward: 22.0, Epsilon: 0.91\n",
      "Episode 19/500, Total Reward: 14.0, Epsilon: 0.91\n",
      "Episode 20/500, Total Reward: 18.0, Epsilon: 0.90\n",
      "Episode 21/500, Total Reward: 22.0, Epsilon: 0.90\n",
      "Episode 22/500, Total Reward: 19.0, Epsilon: 0.90\n",
      "Episode 23/500, Total Reward: 23.0, Epsilon: 0.89\n",
      "Episode 24/500, Total Reward: 15.0, Epsilon: 0.89\n",
      "Episode 25/500, Total Reward: 13.0, Epsilon: 0.88\n",
      "Episode 26/500, Total Reward: 13.0, Epsilon: 0.88\n",
      "Episode 27/500, Total Reward: 24.0, Epsilon: 0.87\n",
      "Episode 28/500, Total Reward: 19.0, Epsilon: 0.87\n",
      "Episode 29/500, Total Reward: 56.0, Epsilon: 0.86\n",
      "Episode 30/500, Total Reward: 14.0, Epsilon: 0.86\n",
      "Episode 31/500, Total Reward: 19.0, Epsilon: 0.86\n",
      "Episode 32/500, Total Reward: 13.0, Epsilon: 0.85\n",
      "Episode 33/500, Total Reward: 21.0, Epsilon: 0.85\n",
      "Episode 34/500, Total Reward: 15.0, Epsilon: 0.84\n",
      "Episode 35/500, Total Reward: 37.0, Epsilon: 0.84\n",
      "Episode 36/500, Total Reward: 31.0, Epsilon: 0.83\n",
      "Episode 37/500, Total Reward: 25.0, Epsilon: 0.83\n",
      "Episode 38/500, Total Reward: 39.0, Epsilon: 0.83\n",
      "Episode 39/500, Total Reward: 10.0, Epsilon: 0.82\n",
      "Episode 40/500, Total Reward: 33.0, Epsilon: 0.82\n",
      "Episode 41/500, Total Reward: 35.0, Epsilon: 0.81\n",
      "Episode 42/500, Total Reward: 38.0, Epsilon: 0.81\n",
      "Episode 43/500, Total Reward: 13.0, Epsilon: 0.81\n",
      "Episode 44/500, Total Reward: 30.0, Epsilon: 0.80\n",
      "Episode 45/500, Total Reward: 44.0, Epsilon: 0.80\n",
      "Episode 46/500, Total Reward: 14.0, Epsilon: 0.79\n",
      "Episode 47/500, Total Reward: 36.0, Epsilon: 0.79\n",
      "Episode 48/500, Total Reward: 18.0, Epsilon: 0.79\n",
      "Episode 49/500, Total Reward: 16.0, Epsilon: 0.78\n",
      "Episode 50/500, Total Reward: 34.0, Epsilon: 0.78\n",
      "Episode 51/500, Total Reward: 24.0, Epsilon: 0.77\n",
      "Episode 52/500, Total Reward: 50.0, Epsilon: 0.77\n",
      "Episode 53/500, Total Reward: 14.0, Epsilon: 0.77\n",
      "Episode 54/500, Total Reward: 37.0, Epsilon: 0.76\n",
      "Episode 55/500, Total Reward: 22.0, Epsilon: 0.76\n",
      "Episode 56/500, Total Reward: 22.0, Epsilon: 0.76\n",
      "Episode 57/500, Total Reward: 26.0, Epsilon: 0.75\n",
      "Episode 58/500, Total Reward: 10.0, Epsilon: 0.75\n",
      "Episode 59/500, Total Reward: 29.0, Epsilon: 0.74\n",
      "Episode 60/500, Total Reward: 62.0, Epsilon: 0.74\n",
      "Episode 61/500, Total Reward: 21.0, Epsilon: 0.74\n",
      "Episode 62/500, Total Reward: 33.0, Epsilon: 0.73\n",
      "Episode 63/500, Total Reward: 62.0, Epsilon: 0.73\n",
      "Episode 64/500, Total Reward: 29.0, Epsilon: 0.73\n",
      "Episode 65/500, Total Reward: 43.0, Epsilon: 0.72\n",
      "Episode 66/500, Total Reward: 46.0, Epsilon: 0.72\n",
      "Episode 67/500, Total Reward: 14.0, Epsilon: 0.71\n",
      "Episode 68/500, Total Reward: 77.0, Epsilon: 0.71\n",
      "Episode 69/500, Total Reward: 29.0, Epsilon: 0.71\n",
      "Episode 70/500, Total Reward: 41.0, Epsilon: 0.70\n",
      "Episode 71/500, Total Reward: 63.0, Epsilon: 0.70\n",
      "Episode 72/500, Total Reward: 81.0, Epsilon: 0.70\n",
      "Episode 73/500, Total Reward: 27.0, Epsilon: 0.69\n",
      "Episode 74/500, Total Reward: 212.0, Epsilon: 0.69\n",
      "Episode 75/500, Total Reward: 60.0, Epsilon: 0.69\n",
      "Episode 76/500, Total Reward: 91.0, Epsilon: 0.68\n",
      "Episode 77/500, Total Reward: 35.0, Epsilon: 0.68\n",
      "Episode 78/500, Total Reward: 124.0, Epsilon: 0.68\n",
      "Episode 79/500, Total Reward: 89.0, Epsilon: 0.67\n",
      "Episode 80/500, Total Reward: 19.0, Epsilon: 0.67\n",
      "Episode 81/500, Total Reward: 19.0, Epsilon: 0.67\n",
      "Episode 82/500, Total Reward: 51.0, Epsilon: 0.66\n",
      "Episode 83/500, Total Reward: 23.0, Epsilon: 0.66\n",
      "Episode 84/500, Total Reward: 22.0, Epsilon: 0.66\n",
      "Episode 85/500, Total Reward: 79.0, Epsilon: 0.65\n",
      "Episode 86/500, Total Reward: 14.0, Epsilon: 0.65\n",
      "Episode 87/500, Total Reward: 29.0, Epsilon: 0.65\n",
      "Episode 88/500, Total Reward: 20.0, Epsilon: 0.64\n",
      "Episode 89/500, Total Reward: 16.0, Epsilon: 0.64\n",
      "Episode 90/500, Total Reward: 32.0, Epsilon: 0.64\n",
      "Episode 91/500, Total Reward: 59.0, Epsilon: 0.63\n",
      "Episode 92/500, Total Reward: 36.0, Epsilon: 0.63\n",
      "Episode 93/500, Total Reward: 89.0, Epsilon: 0.63\n",
      "Episode 94/500, Total Reward: 22.0, Epsilon: 0.62\n",
      "Episode 95/500, Total Reward: 18.0, Epsilon: 0.62\n",
      "Episode 96/500, Total Reward: 186.0, Epsilon: 0.62\n",
      "Episode 97/500, Total Reward: 94.0, Epsilon: 0.61\n",
      "Episode 98/500, Total Reward: 73.0, Epsilon: 0.61\n",
      "Episode 99/500, Total Reward: 154.0, Epsilon: 0.61\n",
      "Episode 100/500, Total Reward: 20.0, Epsilon: 0.61\n",
      "Episode 101/500, Total Reward: 149.0, Epsilon: 0.60\n",
      "Episode 102/500, Total Reward: 20.0, Epsilon: 0.60\n",
      "Episode 103/500, Total Reward: 19.0, Epsilon: 0.60\n",
      "Episode 104/500, Total Reward: 129.0, Epsilon: 0.59\n",
      "Episode 105/500, Total Reward: 70.0, Epsilon: 0.59\n",
      "Episode 106/500, Total Reward: 199.0, Epsilon: 0.59\n",
      "Episode 107/500, Total Reward: 52.0, Epsilon: 0.58\n",
      "Episode 108/500, Total Reward: 56.0, Epsilon: 0.58\n",
      "Episode 109/500, Total Reward: 96.0, Epsilon: 0.58\n",
      "Episode 110/500, Total Reward: 114.0, Epsilon: 0.58\n",
      "Episode 111/500, Total Reward: 236.0, Epsilon: 0.57\n",
      "Episode 112/500, Total Reward: 183.0, Epsilon: 0.57\n",
      "Episode 113/500, Total Reward: 135.0, Epsilon: 0.57\n",
      "Episode 114/500, Total Reward: 78.0, Epsilon: 0.56\n",
      "Episode 115/500, Total Reward: 40.0, Epsilon: 0.56\n",
      "Episode 116/500, Total Reward: 245.0, Epsilon: 0.56\n",
      "Episode 117/500, Total Reward: 55.0, Epsilon: 0.56\n",
      "Episode 118/500, Total Reward: 24.0, Epsilon: 0.55\n",
      "Episode 119/500, Total Reward: 94.0, Epsilon: 0.55\n",
      "Episode 120/500, Total Reward: 22.0, Epsilon: 0.55\n",
      "Episode 121/500, Total Reward: 67.0, Epsilon: 0.55\n",
      "Episode 122/500, Total Reward: 37.0, Epsilon: 0.54\n",
      "Episode 123/500, Total Reward: 60.0, Epsilon: 0.54\n",
      "Episode 124/500, Total Reward: 60.0, Epsilon: 0.54\n",
      "Episode 125/500, Total Reward: 82.0, Epsilon: 0.53\n",
      "Episode 126/500, Total Reward: 60.0, Epsilon: 0.53\n",
      "Episode 127/500, Total Reward: 39.0, Epsilon: 0.53\n",
      "Episode 128/500, Total Reward: 193.0, Epsilon: 0.53\n",
      "Episode 129/500, Total Reward: 74.0, Epsilon: 0.52\n",
      "Episode 130/500, Total Reward: 214.0, Epsilon: 0.52\n",
      "Episode 131/500, Total Reward: 48.0, Epsilon: 0.52\n",
      "Episode 132/500, Total Reward: 198.0, Epsilon: 0.52\n",
      "Episode 133/500, Total Reward: 69.0, Epsilon: 0.51\n",
      "Episode 134/500, Total Reward: 67.0, Epsilon: 0.51\n",
      "Episode 135/500, Total Reward: 33.0, Epsilon: 0.51\n",
      "Episode 136/500, Total Reward: 257.0, Epsilon: 0.51\n",
      "Episode 137/500, Total Reward: 236.0, Epsilon: 0.50\n",
      "Episode 138/500, Total Reward: 239.0, Epsilon: 0.50\n",
      "Episode 139/500, Total Reward: 34.0, Epsilon: 0.50\n",
      "Episode 140/500, Total Reward: 216.0, Epsilon: 0.50\n",
      "Episode 141/500, Total Reward: 21.0, Epsilon: 0.49\n",
      "Episode 142/500, Total Reward: 57.0, Epsilon: 0.49\n",
      "Episode 143/500, Total Reward: 34.0, Epsilon: 0.49\n",
      "Episode 144/500, Total Reward: 24.0, Epsilon: 0.49\n",
      "Episode 145/500, Total Reward: 146.0, Epsilon: 0.48\n",
      "Episode 146/500, Total Reward: 265.0, Epsilon: 0.48\n",
      "Episode 147/500, Total Reward: 193.0, Epsilon: 0.48\n",
      "Episode 148/500, Total Reward: 96.0, Epsilon: 0.48\n",
      "Episode 149/500, Total Reward: 81.0, Epsilon: 0.47\n",
      "Episode 150/500, Total Reward: 78.0, Epsilon: 0.47\n",
      "Episode 151/500, Total Reward: 120.0, Epsilon: 0.47\n",
      "Episode 152/500, Total Reward: 173.0, Epsilon: 0.47\n",
      "Episode 153/500, Total Reward: 198.0, Epsilon: 0.46\n",
      "Episode 154/500, Total Reward: 262.0, Epsilon: 0.46\n",
      "Episode 155/500, Total Reward: 107.0, Epsilon: 0.46\n",
      "Episode 156/500, Total Reward: 198.0, Epsilon: 0.46\n",
      "Episode 157/500, Total Reward: 218.0, Epsilon: 0.46\n",
      "Episode 158/500, Total Reward: 80.0, Epsilon: 0.45\n",
      "Episode 159/500, Total Reward: 308.0, Epsilon: 0.45\n",
      "Episode 160/500, Total Reward: 198.0, Epsilon: 0.45\n",
      "Episode 161/500, Total Reward: 30.0, Epsilon: 0.45\n",
      "Episode 162/500, Total Reward: 247.0, Epsilon: 0.44\n",
      "Episode 163/500, Total Reward: 118.0, Epsilon: 0.44\n",
      "Episode 164/500, Total Reward: 39.0, Epsilon: 0.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 165/500, Total Reward: 112.0, Epsilon: 0.44\n",
      "Episode 166/500, Total Reward: 98.0, Epsilon: 0.44\n",
      "Episode 167/500, Total Reward: 264.0, Epsilon: 0.43\n",
      "Episode 168/500, Total Reward: 147.0, Epsilon: 0.43\n",
      "Episode 169/500, Total Reward: 237.0, Epsilon: 0.43\n",
      "Episode 170/500, Total Reward: 87.0, Epsilon: 0.43\n",
      "Episode 171/500, Total Reward: 233.0, Epsilon: 0.42\n",
      "Episode 172/500, Total Reward: 164.0, Epsilon: 0.42\n",
      "Episode 173/500, Total Reward: 230.0, Epsilon: 0.42\n",
      "Episode 174/500, Total Reward: 182.0, Epsilon: 0.42\n",
      "Episode 175/500, Total Reward: 232.0, Epsilon: 0.42\n",
      "Episode 176/500, Total Reward: 363.0, Epsilon: 0.41\n",
      "Episode 177/500, Total Reward: 172.0, Epsilon: 0.41\n",
      "Episode 178/500, Total Reward: 234.0, Epsilon: 0.41\n",
      "Episode 179/500, Total Reward: 252.0, Epsilon: 0.41\n",
      "Episode 180/500, Total Reward: 209.0, Epsilon: 0.41\n",
      "Episode 181/500, Total Reward: 214.0, Epsilon: 0.40\n",
      "Episode 182/500, Total Reward: 178.0, Epsilon: 0.40\n",
      "Episode 183/500, Total Reward: 197.0, Epsilon: 0.40\n",
      "Episode 184/500, Total Reward: 175.0, Epsilon: 0.40\n",
      "Episode 185/500, Total Reward: 30.0, Epsilon: 0.40\n",
      "Episode 186/500, Total Reward: 144.0, Epsilon: 0.39\n",
      "Episode 187/500, Total Reward: 140.0, Epsilon: 0.39\n",
      "Episode 188/500, Total Reward: 15.0, Epsilon: 0.39\n",
      "Episode 189/500, Total Reward: 224.0, Epsilon: 0.39\n",
      "Episode 190/500, Total Reward: 63.0, Epsilon: 0.39\n",
      "Episode 191/500, Total Reward: 158.0, Epsilon: 0.38\n",
      "Episode 192/500, Total Reward: 248.0, Epsilon: 0.38\n",
      "Episode 193/500, Total Reward: 199.0, Epsilon: 0.38\n",
      "Episode 194/500, Total Reward: 156.0, Epsilon: 0.38\n",
      "Episode 195/500, Total Reward: 34.0, Epsilon: 0.38\n",
      "Episode 196/500, Total Reward: 179.0, Epsilon: 0.37\n",
      "Episode 197/500, Total Reward: 208.0, Epsilon: 0.37\n",
      "Episode 198/500, Total Reward: 14.0, Epsilon: 0.37\n",
      "Episode 199/500, Total Reward: 197.0, Epsilon: 0.37\n",
      "Episode 200/500, Total Reward: 189.0, Epsilon: 0.37\n",
      "Episode 201/500, Total Reward: 186.0, Epsilon: 0.37\n",
      "Episode 202/500, Total Reward: 148.0, Epsilon: 0.36\n",
      "Episode 203/500, Total Reward: 160.0, Epsilon: 0.36\n",
      "Episode 204/500, Total Reward: 117.0, Epsilon: 0.36\n",
      "Episode 205/500, Total Reward: 161.0, Epsilon: 0.36\n",
      "Episode 206/500, Total Reward: 157.0, Epsilon: 0.36\n",
      "Episode 207/500, Total Reward: 179.0, Epsilon: 0.35\n",
      "Episode 208/500, Total Reward: 186.0, Epsilon: 0.35\n",
      "Episode 209/500, Total Reward: 238.0, Epsilon: 0.35\n",
      "Episode 210/500, Total Reward: 43.0, Epsilon: 0.35\n",
      "Episode 211/500, Total Reward: 190.0, Epsilon: 0.35\n",
      "Episode 212/500, Total Reward: 25.0, Epsilon: 0.35\n",
      "Episode 213/500, Total Reward: 188.0, Epsilon: 0.34\n",
      "Episode 214/500, Total Reward: 153.0, Epsilon: 0.34\n",
      "Episode 215/500, Total Reward: 130.0, Epsilon: 0.34\n",
      "Episode 216/500, Total Reward: 219.0, Epsilon: 0.34\n",
      "Episode 217/500, Total Reward: 72.0, Epsilon: 0.34\n",
      "Episode 218/500, Total Reward: 174.0, Epsilon: 0.34\n",
      "Episode 219/500, Total Reward: 153.0, Epsilon: 0.33\n",
      "Episode 220/500, Total Reward: 158.0, Epsilon: 0.33\n",
      "Episode 221/500, Total Reward: 131.0, Epsilon: 0.33\n",
      "Episode 222/500, Total Reward: 189.0, Epsilon: 0.33\n",
      "Episode 223/500, Total Reward: 23.0, Epsilon: 0.33\n",
      "Episode 224/500, Total Reward: 149.0, Epsilon: 0.33\n",
      "Episode 225/500, Total Reward: 38.0, Epsilon: 0.32\n",
      "Episode 226/500, Total Reward: 158.0, Epsilon: 0.32\n",
      "Episode 227/500, Total Reward: 179.0, Epsilon: 0.32\n",
      "Episode 228/500, Total Reward: 157.0, Epsilon: 0.32\n",
      "Episode 229/500, Total Reward: 74.0, Epsilon: 0.32\n",
      "Episode 230/500, Total Reward: 170.0, Epsilon: 0.32\n",
      "Episode 231/500, Total Reward: 46.0, Epsilon: 0.31\n",
      "Episode 232/500, Total Reward: 17.0, Epsilon: 0.31\n",
      "Episode 233/500, Total Reward: 169.0, Epsilon: 0.31\n",
      "Episode 234/500, Total Reward: 130.0, Epsilon: 0.31\n",
      "Episode 235/500, Total Reward: 150.0, Epsilon: 0.31\n",
      "Episode 236/500, Total Reward: 36.0, Epsilon: 0.31\n",
      "Episode 237/500, Total Reward: 67.0, Epsilon: 0.30\n",
      "Episode 238/500, Total Reward: 178.0, Epsilon: 0.30\n",
      "Episode 239/500, Total Reward: 13.0, Epsilon: 0.30\n",
      "Episode 240/500, Total Reward: 80.0, Epsilon: 0.30\n",
      "Episode 241/500, Total Reward: 152.0, Epsilon: 0.30\n",
      "Episode 242/500, Total Reward: 17.0, Epsilon: 0.30\n",
      "Episode 243/500, Total Reward: 37.0, Epsilon: 0.30\n",
      "Episode 244/500, Total Reward: 91.0, Epsilon: 0.29\n",
      "Episode 245/500, Total Reward: 168.0, Epsilon: 0.29\n",
      "Episode 246/500, Total Reward: 157.0, Epsilon: 0.29\n",
      "Episode 247/500, Total Reward: 173.0, Epsilon: 0.29\n",
      "Episode 248/500, Total Reward: 72.0, Epsilon: 0.29\n",
      "Episode 249/500, Total Reward: 19.0, Epsilon: 0.29\n",
      "Episode 250/500, Total Reward: 129.0, Epsilon: 0.29\n",
      "Episode 251/500, Total Reward: 49.0, Epsilon: 0.28\n",
      "Episode 252/500, Total Reward: 140.0, Epsilon: 0.28\n",
      "Episode 253/500, Total Reward: 203.0, Epsilon: 0.28\n",
      "Episode 254/500, Total Reward: 187.0, Epsilon: 0.28\n",
      "Episode 255/500, Total Reward: 188.0, Epsilon: 0.28\n",
      "Episode 256/500, Total Reward: 166.0, Epsilon: 0.28\n",
      "Episode 257/500, Total Reward: 112.0, Epsilon: 0.28\n",
      "Episode 258/500, Total Reward: 20.0, Epsilon: 0.27\n",
      "Episode 259/500, Total Reward: 256.0, Epsilon: 0.27\n",
      "Episode 260/500, Total Reward: 90.0, Epsilon: 0.27\n",
      "Episode 261/500, Total Reward: 148.0, Epsilon: 0.27\n",
      "Episode 262/500, Total Reward: 52.0, Epsilon: 0.27\n",
      "Episode 263/500, Total Reward: 166.0, Epsilon: 0.27\n",
      "Episode 264/500, Total Reward: 188.0, Epsilon: 0.27\n",
      "Episode 265/500, Total Reward: 260.0, Epsilon: 0.26\n",
      "Episode 266/500, Total Reward: 271.0, Epsilon: 0.26\n",
      "Episode 267/500, Total Reward: 300.0, Epsilon: 0.26\n",
      "Episode 268/500, Total Reward: 250.0, Epsilon: 0.26\n",
      "Episode 269/500, Total Reward: 128.0, Epsilon: 0.26\n",
      "Episode 270/500, Total Reward: 208.0, Epsilon: 0.26\n",
      "Episode 271/500, Total Reward: 403.0, Epsilon: 0.26\n",
      "Episode 272/500, Total Reward: 312.0, Epsilon: 0.26\n",
      "Episode 273/500, Total Reward: 206.0, Epsilon: 0.25\n",
      "Episode 274/500, Total Reward: 269.0, Epsilon: 0.25\n",
      "Episode 275/500, Total Reward: 197.0, Epsilon: 0.25\n",
      "Episode 276/500, Total Reward: 234.0, Epsilon: 0.25\n",
      "Episode 277/500, Total Reward: 160.0, Epsilon: 0.25\n",
      "Episode 278/500, Total Reward: 117.0, Epsilon: 0.25\n",
      "Episode 279/500, Total Reward: 277.0, Epsilon: 0.25\n",
      "Episode 280/500, Total Reward: 264.0, Epsilon: 0.25\n",
      "Episode 281/500, Total Reward: 2496.0, Epsilon: 0.24\n",
      "Episode 282/500, Total Reward: 16.0, Epsilon: 0.24\n",
      "Episode 283/500, Total Reward: 40.0, Epsilon: 0.24\n",
      "Episode 284/500, Total Reward: 542.0, Epsilon: 0.24\n",
      "Episode 285/500, Total Reward: 137.0, Epsilon: 0.24\n",
      "Episode 286/500, Total Reward: 135.0, Epsilon: 0.24\n",
      "Episode 287/500, Total Reward: 241.0, Epsilon: 0.24\n",
      "Episode 288/500, Total Reward: 170.0, Epsilon: 0.24\n",
      "Episode 289/500, Total Reward: 244.0, Epsilon: 0.23\n",
      "Episode 290/500, Total Reward: 132.0, Epsilon: 0.23\n",
      "Episode 291/500, Total Reward: 75.0, Epsilon: 0.23\n",
      "Episode 292/500, Total Reward: 148.0, Epsilon: 0.23\n",
      "Episode 293/500, Total Reward: 179.0, Epsilon: 0.23\n",
      "Episode 294/500, Total Reward: 242.0, Epsilon: 0.23\n",
      "Episode 295/500, Total Reward: 175.0, Epsilon: 0.23\n",
      "Episode 296/500, Total Reward: 315.0, Epsilon: 0.23\n",
      "Episode 297/500, Total Reward: 178.0, Epsilon: 0.23\n",
      "Episode 298/500, Total Reward: 367.0, Epsilon: 0.22\n",
      "Episode 299/500, Total Reward: 233.0, Epsilon: 0.22\n",
      "Episode 300/500, Total Reward: 88.0, Epsilon: 0.22\n",
      "Episode 301/500, Total Reward: 123.0, Epsilon: 0.22\n",
      "Episode 302/500, Total Reward: 177.0, Epsilon: 0.22\n",
      "Episode 303/500, Total Reward: 180.0, Epsilon: 0.22\n",
      "Episode 304/500, Total Reward: 160.0, Epsilon: 0.22\n",
      "Episode 305/500, Total Reward: 320.0, Epsilon: 0.22\n",
      "Episode 306/500, Total Reward: 38.0, Epsilon: 0.22\n",
      "Episode 307/500, Total Reward: 153.0, Epsilon: 0.21\n",
      "Episode 308/500, Total Reward: 284.0, Epsilon: 0.21\n",
      "Episode 309/500, Total Reward: 130.0, Epsilon: 0.21\n",
      "Episode 310/500, Total Reward: 280.0, Epsilon: 0.21\n",
      "Episode 311/500, Total Reward: 122.0, Epsilon: 0.21\n",
      "Episode 312/500, Total Reward: 102.0, Epsilon: 0.21\n",
      "Episode 313/500, Total Reward: 97.0, Epsilon: 0.21\n",
      "Episode 314/500, Total Reward: 116.0, Epsilon: 0.21\n",
      "Episode 315/500, Total Reward: 99.0, Epsilon: 0.21\n",
      "Episode 316/500, Total Reward: 110.0, Epsilon: 0.21\n",
      "Episode 317/500, Total Reward: 97.0, Epsilon: 0.20\n",
      "Episode 318/500, Total Reward: 107.0, Epsilon: 0.20\n",
      "Episode 319/500, Total Reward: 100.0, Epsilon: 0.20\n",
      "Episode 320/500, Total Reward: 94.0, Epsilon: 0.20\n",
      "Episode 321/500, Total Reward: 105.0, Epsilon: 0.20\n",
      "Episode 322/500, Total Reward: 98.0, Epsilon: 0.20\n",
      "Episode 323/500, Total Reward: 104.0, Epsilon: 0.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 324/500, Total Reward: 99.0, Epsilon: 0.20\n",
      "Episode 325/500, Total Reward: 98.0, Epsilon: 0.20\n",
      "Episode 326/500, Total Reward: 102.0, Epsilon: 0.20\n",
      "Episode 327/500, Total Reward: 38.0, Epsilon: 0.19\n",
      "Episode 328/500, Total Reward: 41.0, Epsilon: 0.19\n",
      "Episode 329/500, Total Reward: 103.0, Epsilon: 0.19\n",
      "Episode 330/500, Total Reward: 100.0, Epsilon: 0.19\n",
      "Episode 331/500, Total Reward: 96.0, Epsilon: 0.19\n",
      "Episode 332/500, Total Reward: 63.0, Epsilon: 0.19\n",
      "Episode 333/500, Total Reward: 113.0, Epsilon: 0.19\n",
      "Episode 334/500, Total Reward: 21.0, Epsilon: 0.19\n",
      "Episode 335/500, Total Reward: 120.0, Epsilon: 0.19\n",
      "Episode 336/500, Total Reward: 98.0, Epsilon: 0.19\n",
      "Episode 337/500, Total Reward: 104.0, Epsilon: 0.18\n",
      "Episode 338/500, Total Reward: 110.0, Epsilon: 0.18\n",
      "Episode 339/500, Total Reward: 102.0, Epsilon: 0.18\n",
      "Episode 340/500, Total Reward: 106.0, Epsilon: 0.18\n",
      "Episode 341/500, Total Reward: 37.0, Epsilon: 0.18\n",
      "Episode 342/500, Total Reward: 111.0, Epsilon: 0.18\n",
      "Episode 343/500, Total Reward: 270.0, Epsilon: 0.18\n",
      "Episode 344/500, Total Reward: 120.0, Epsilon: 0.18\n",
      "Episode 345/500, Total Reward: 50.0, Epsilon: 0.18\n",
      "Episode 346/500, Total Reward: 110.0, Epsilon: 0.18\n",
      "Episode 347/500, Total Reward: 114.0, Epsilon: 0.18\n",
      "Episode 348/500, Total Reward: 102.0, Epsilon: 0.17\n",
      "Episode 349/500, Total Reward: 37.0, Epsilon: 0.17\n",
      "Episode 350/500, Total Reward: 120.0, Epsilon: 0.17\n",
      "Episode 351/500, Total Reward: 61.0, Epsilon: 0.17\n",
      "Episode 352/500, Total Reward: 107.0, Epsilon: 0.17\n",
      "Episode 353/500, Total Reward: 89.0, Epsilon: 0.17\n",
      "Episode 354/500, Total Reward: 96.0, Epsilon: 0.17\n",
      "Episode 355/500, Total Reward: 103.0, Epsilon: 0.17\n",
      "Episode 356/500, Total Reward: 20.0, Epsilon: 0.17\n",
      "Episode 357/500, Total Reward: 106.0, Epsilon: 0.17\n",
      "Episode 358/500, Total Reward: 83.0, Epsilon: 0.17\n",
      "Episode 359/500, Total Reward: 93.0, Epsilon: 0.17\n",
      "Episode 360/500, Total Reward: 55.0, Epsilon: 0.16\n",
      "Episode 361/500, Total Reward: 95.0, Epsilon: 0.16\n",
      "Episode 362/500, Total Reward: 97.0, Epsilon: 0.16\n",
      "Episode 363/500, Total Reward: 66.0, Epsilon: 0.16\n",
      "Episode 364/500, Total Reward: 107.0, Epsilon: 0.16\n",
      "Episode 365/500, Total Reward: 135.0, Epsilon: 0.16\n",
      "Episode 366/500, Total Reward: 149.0, Epsilon: 0.16\n",
      "Episode 367/500, Total Reward: 156.0, Epsilon: 0.16\n",
      "Episode 368/500, Total Reward: 99.0, Epsilon: 0.16\n",
      "Episode 369/500, Total Reward: 132.0, Epsilon: 0.16\n",
      "Episode 370/500, Total Reward: 121.0, Epsilon: 0.16\n",
      "Episode 371/500, Total Reward: 105.0, Epsilon: 0.16\n",
      "Episode 372/500, Total Reward: 58.0, Epsilon: 0.15\n",
      "Episode 373/500, Total Reward: 33.0, Epsilon: 0.15\n",
      "Episode 374/500, Total Reward: 68.0, Epsilon: 0.15\n",
      "Episode 375/500, Total Reward: 168.0, Epsilon: 0.15\n",
      "Episode 376/500, Total Reward: 112.0, Epsilon: 0.15\n",
      "Episode 377/500, Total Reward: 108.0, Epsilon: 0.15\n",
      "Episode 378/500, Total Reward: 164.0, Epsilon: 0.15\n",
      "Episode 379/500, Total Reward: 203.0, Epsilon: 0.15\n",
      "Episode 380/500, Total Reward: 103.0, Epsilon: 0.15\n",
      "Episode 381/500, Total Reward: 115.0, Epsilon: 0.15\n",
      "Episode 382/500, Total Reward: 228.0, Epsilon: 0.15\n",
      "Episode 383/500, Total Reward: 238.0, Epsilon: 0.15\n",
      "Episode 384/500, Total Reward: 244.0, Epsilon: 0.15\n",
      "Episode 385/500, Total Reward: 145.0, Epsilon: 0.15\n",
      "Episode 386/500, Total Reward: 262.0, Epsilon: 0.14\n",
      "Episode 387/500, Total Reward: 110.0, Epsilon: 0.14\n",
      "Episode 388/500, Total Reward: 167.0, Epsilon: 0.14\n",
      "Episode 389/500, Total Reward: 111.0, Epsilon: 0.14\n",
      "Episode 390/500, Total Reward: 269.0, Epsilon: 0.14\n",
      "Episode 391/500, Total Reward: 317.0, Epsilon: 0.14\n",
      "Episode 392/500, Total Reward: 163.0, Epsilon: 0.14\n",
      "Episode 393/500, Total Reward: 151.0, Epsilon: 0.14\n",
      "Episode 394/500, Total Reward: 142.0, Epsilon: 0.14\n",
      "Episode 395/500, Total Reward: 189.0, Epsilon: 0.14\n",
      "Episode 396/500, Total Reward: 124.0, Epsilon: 0.14\n",
      "Episode 397/500, Total Reward: 143.0, Epsilon: 0.14\n",
      "Episode 398/500, Total Reward: 133.0, Epsilon: 0.14\n",
      "Episode 399/500, Total Reward: 132.0, Epsilon: 0.14\n",
      "Episode 400/500, Total Reward: 120.0, Epsilon: 0.13\n",
      "Episode 401/500, Total Reward: 126.0, Epsilon: 0.13\n",
      "Episode 402/500, Total Reward: 15.0, Epsilon: 0.13\n",
      "Episode 403/500, Total Reward: 122.0, Epsilon: 0.13\n",
      "Episode 404/500, Total Reward: 114.0, Epsilon: 0.13\n",
      "Episode 405/500, Total Reward: 117.0, Epsilon: 0.13\n",
      "Episode 406/500, Total Reward: 117.0, Epsilon: 0.13\n",
      "Episode 407/500, Total Reward: 109.0, Epsilon: 0.13\n",
      "Episode 408/500, Total Reward: 110.0, Epsilon: 0.13\n",
      "Episode 409/500, Total Reward: 116.0, Epsilon: 0.13\n",
      "Episode 410/500, Total Reward: 118.0, Epsilon: 0.13\n",
      "Episode 411/500, Total Reward: 107.0, Epsilon: 0.13\n",
      "Episode 412/500, Total Reward: 106.0, Epsilon: 0.13\n",
      "Episode 413/500, Total Reward: 106.0, Epsilon: 0.13\n",
      "Episode 414/500, Total Reward: 113.0, Epsilon: 0.13\n",
      "Episode 415/500, Total Reward: 99.0, Epsilon: 0.12\n",
      "Episode 416/500, Total Reward: 17.0, Epsilon: 0.12\n",
      "Episode 417/500, Total Reward: 101.0, Epsilon: 0.12\n",
      "Episode 418/500, Total Reward: 105.0, Epsilon: 0.12\n",
      "Episode 419/500, Total Reward: 106.0, Epsilon: 0.12\n",
      "Episode 420/500, Total Reward: 108.0, Epsilon: 0.12\n",
      "Episode 421/500, Total Reward: 102.0, Epsilon: 0.12\n",
      "Episode 422/500, Total Reward: 111.0, Epsilon: 0.12\n",
      "Episode 423/500, Total Reward: 107.0, Epsilon: 0.12\n",
      "Episode 424/500, Total Reward: 105.0, Epsilon: 0.12\n",
      "Episode 425/500, Total Reward: 106.0, Epsilon: 0.12\n",
      "Episode 426/500, Total Reward: 13.0, Epsilon: 0.12\n",
      "Episode 427/500, Total Reward: 106.0, Epsilon: 0.12\n",
      "Episode 428/500, Total Reward: 106.0, Epsilon: 0.12\n",
      "Episode 429/500, Total Reward: 101.0, Epsilon: 0.12\n",
      "Episode 430/500, Total Reward: 101.0, Epsilon: 0.12\n",
      "Episode 431/500, Total Reward: 107.0, Epsilon: 0.12\n",
      "Episode 432/500, Total Reward: 107.0, Epsilon: 0.11\n",
      "Episode 433/500, Total Reward: 110.0, Epsilon: 0.11\n",
      "Episode 434/500, Total Reward: 104.0, Epsilon: 0.11\n",
      "Episode 435/500, Total Reward: 107.0, Epsilon: 0.11\n",
      "Episode 436/500, Total Reward: 100.0, Epsilon: 0.11\n",
      "Episode 437/500, Total Reward: 102.0, Epsilon: 0.11\n",
      "Episode 438/500, Total Reward: 100.0, Epsilon: 0.11\n",
      "Episode 439/500, Total Reward: 101.0, Epsilon: 0.11\n",
      "Episode 440/500, Total Reward: 103.0, Epsilon: 0.11\n",
      "Episode 441/500, Total Reward: 15.0, Epsilon: 0.11\n",
      "Episode 442/500, Total Reward: 110.0, Epsilon: 0.11\n",
      "Episode 443/500, Total Reward: 101.0, Epsilon: 0.11\n",
      "Episode 444/500, Total Reward: 101.0, Epsilon: 0.11\n",
      "Episode 445/500, Total Reward: 104.0, Epsilon: 0.11\n",
      "Episode 446/500, Total Reward: 104.0, Epsilon: 0.11\n",
      "Episode 447/500, Total Reward: 99.0, Epsilon: 0.11\n",
      "Episode 448/500, Total Reward: 107.0, Epsilon: 0.11\n",
      "Episode 449/500, Total Reward: 103.0, Epsilon: 0.11\n",
      "Episode 450/500, Total Reward: 97.0, Epsilon: 0.10\n",
      "Episode 451/500, Total Reward: 98.0, Epsilon: 0.10\n",
      "Episode 452/500, Total Reward: 104.0, Epsilon: 0.10\n",
      "Episode 453/500, Total Reward: 103.0, Epsilon: 0.10\n",
      "Episode 454/500, Total Reward: 103.0, Epsilon: 0.10\n",
      "Episode 455/500, Total Reward: 99.0, Epsilon: 0.10\n",
      "Episode 456/500, Total Reward: 100.0, Epsilon: 0.10\n",
      "Episode 457/500, Total Reward: 99.0, Epsilon: 0.10\n",
      "Episode 458/500, Total Reward: 99.0, Epsilon: 0.10\n",
      "Episode 459/500, Total Reward: 100.0, Epsilon: 0.10\n",
      "Episode 460/500, Total Reward: 99.0, Epsilon: 0.10\n",
      "Episode 461/500, Total Reward: 100.0, Epsilon: 0.10\n",
      "Episode 462/500, Total Reward: 101.0, Epsilon: 0.10\n",
      "Episode 463/500, Total Reward: 103.0, Epsilon: 0.10\n",
      "Episode 464/500, Total Reward: 106.0, Epsilon: 0.10\n",
      "Episode 465/500, Total Reward: 107.0, Epsilon: 0.10\n",
      "Episode 466/500, Total Reward: 100.0, Epsilon: 0.10\n",
      "Episode 467/500, Total Reward: 101.0, Epsilon: 0.10\n",
      "Episode 468/500, Total Reward: 104.0, Epsilon: 0.10\n",
      "Episode 469/500, Total Reward: 106.0, Epsilon: 0.10\n",
      "Episode 470/500, Total Reward: 95.0, Epsilon: 0.09\n",
      "Episode 471/500, Total Reward: 106.0, Epsilon: 0.09\n",
      "Episode 472/500, Total Reward: 107.0, Epsilon: 0.09\n",
      "Episode 473/500, Total Reward: 102.0, Epsilon: 0.09\n",
      "Episode 474/500, Total Reward: 105.0, Epsilon: 0.09\n",
      "Episode 475/500, Total Reward: 99.0, Epsilon: 0.09\n",
      "Episode 476/500, Total Reward: 104.0, Epsilon: 0.09\n",
      "Episode 477/500, Total Reward: 106.0, Epsilon: 0.09\n",
      "Episode 478/500, Total Reward: 105.0, Epsilon: 0.09\n",
      "Episode 479/500, Total Reward: 112.0, Epsilon: 0.09\n",
      "Episode 480/500, Total Reward: 100.0, Epsilon: 0.09\n",
      "Episode 481/500, Total Reward: 101.0, Epsilon: 0.09\n",
      "Episode 482/500, Total Reward: 104.0, Epsilon: 0.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 483/500, Total Reward: 30.0, Epsilon: 0.09\n",
      "Episode 484/500, Total Reward: 103.0, Epsilon: 0.09\n",
      "Episode 485/500, Total Reward: 106.0, Epsilon: 0.09\n",
      "Episode 486/500, Total Reward: 101.0, Epsilon: 0.09\n",
      "Episode 487/500, Total Reward: 14.0, Epsilon: 0.09\n",
      "Episode 488/500, Total Reward: 107.0, Epsilon: 0.09\n",
      "Episode 489/500, Total Reward: 104.0, Epsilon: 0.09\n",
      "Episode 490/500, Total Reward: 107.0, Epsilon: 0.09\n",
      "Episode 491/500, Total Reward: 105.0, Epsilon: 0.09\n",
      "Episode 492/500, Total Reward: 106.0, Epsilon: 0.08\n",
      "Episode 493/500, Total Reward: 37.0, Epsilon: 0.08\n",
      "Episode 494/500, Total Reward: 107.0, Epsilon: 0.08\n",
      "Episode 495/500, Total Reward: 102.0, Epsilon: 0.08\n",
      "Episode 496/500, Total Reward: 104.0, Epsilon: 0.08\n",
      "Episode 497/500, Total Reward: 104.0, Epsilon: 0.08\n",
      "Episode 498/500, Total Reward: 108.0, Epsilon: 0.08\n",
      "Episode 499/500, Total Reward: 104.0, Epsilon: 0.08\n",
      "Episode 500/500, Total Reward: 101.0, Epsilon: 0.08\n"
     ]
    }
   ],
   "source": [
    "# Reset environment and initialize state\n",
    "state_dict = env.reset()\n",
    "if isinstance(state_dict, tuple):  # Handle cases where reset returns a tuple\n",
    "    state = state_dict[0]\n",
    "else:\n",
    "    state = state_dict  # Use directly if it's already a simple observation array\n",
    "\n",
    "# Training loop with modified reset handling\n",
    "for episode in range(num_episodes):\n",
    "    # Reset environment and initialize state\n",
    "    state_dict = env.reset()\n",
    "    if isinstance(state_dict, tuple):\n",
    "        state = state_dict[0]\n",
    "    else:\n",
    "        state = state_dict\n",
    "    \n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Select an action\n",
    "        action = select_action(state, epsilon)\n",
    "        \n",
    "        next_state, reward, done, *info = env.step(action)\n",
    "        \n",
    "        # Append experience to replay memory\n",
    "        replay_memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Train the Q-network\n",
    "        train_q_network()\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    # Decay epsilon and update target network\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "    if episode % 10 == 0:\n",
    "        target_network.load_state_dict(q_network.state_dict())\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971adc8f",
   "metadata": {},
   "source": [
    "#### Deep Q-Learning with a Neural Network\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm that learns a Q-value function \n",
    "Q(s,a), which represents the expected cumulative reward for taking action a in state s, and following the optimal policy thereafter.\n",
    "\n",
    "Traditional Q-Learning uses a table to store Q-values for all possible state-action pairs. However, this becomes impractical for environments like CartPole with continuous state spaces (e.g., position, velocity, etc.).\n",
    "In Deep Q-Learning, a neural network approximates the Q-value function. Instead of storing a table, the network takes the state as input and outputs Q-values for all possible actions.\n",
    "Neural Network Design:\n",
    "\n",
    "The neural network has an input layer matching the dimensions of the state space (e.g., 4 for CartPole).\n",
    "It outputs the Q-values for each possible action (2 for CartPole: left and right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c34f60d",
   "metadata": {},
   "source": [
    "#### Exploration-Exploitation Trade-Off via Epsilon-Greedy Policy\n",
    "\n",
    "Exploration means trying new actions to discover potentially better rewards.\n",
    "Exploitation means using the current knowledge (Q-values) to take the action with the highest reward.\n",
    "Epsilon-Greedy Policy:\n",
    "\n",
    "The agent chooses:A random action with probability ϵ (exploration).\n",
    "The action with the highest Q-value with probability 1−ϵ (exploitation).\n",
    "This balance ensures the agent explores new strategies while gradually focusing on the best-known actions.\n",
    "\n",
    "Epsilon Decay:Initially, ϵ is high (e.g., 1.0), encouraging exploration.Over time, ϵ decays (e.g.,ϵ=ϵ×0.995) to reduce exploration as the agent becomes more confident in its learned Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1081c",
   "metadata": {},
   "source": [
    "#### Replay Memory\n",
    "\n",
    "Replay memory stores a buffer of past experiences in the form(state,action,reward,next_state,done).\n",
    "These experiences are randomly sampled during training to update the Q-value function.\n",
    "\n",
    "Breaks Correlation: Consecutive experiences are highly correlated. Random sampling prevents the model from overfitting to recent events.\n",
    "Efficient Data Usage: The agent learns from past experiences multiple times, improving sample efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04750975",
   "metadata": {},
   "source": [
    "#### Target Network\n",
    "\n",
    "In DQL, two neural networks are used:\n",
    "The online network updates Q-values during training.\n",
    "The target network provides stable Q-value targets for training the online network.\n",
    "\n",
    "The Q-value target involves the next state's maximum Q-value, which depends on the same network being trained. This creates a feedback loop and can lead to instability.\n",
    "By freezing the target network’s weights for several training steps, the Q-value targets become more stable, improving convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65240a99",
   "metadata": {},
   "source": [
    "##### Early Episodes (Episode 1/500, Total Reward: 15.0, Epsilon: 0.99):\n",
    "\n",
    "##### Agent’s Performance:\n",
    "In the early episodes, the agent performs poorly, earning low rewards (e.g., 15.0).\n",
    "The agent has not yet learned a good policy and is primarily exploring the environment.\n",
    "High Epsilon (0.99):\n",
    "The agent mostly selects random actions to explore possible strategies and understand the environment dynamics.\n",
    "This randomness often leads to suboptimal decisions.\n",
    "\n",
    "#### Midpoint of Training:(Episode 250/500, Total Reward: 129.0, Epsilon: 0.29)\n",
    "\n",
    "#### Improvement in Performance:\n",
    "As episodes progress, the agent begins learning a better policy through experience replay and Q-value updates.\n",
    "The total rewards gradually increase, indicating that the agent balances the pole for longer durations.\n",
    "Reduced Epsilon:\n",
    "The exploration rate (ϵ) decreases, meaning the agent increasingly relies on its learned Q-values to make decisions instead of random exploration.\n",
    "\n",
    "#### Later Episodes (Episode 500/500, Total Reward: 101.0, Epsilon: 0.08):\n",
    "\n",
    "#### Agent’s Performance:\n",
    "By the end of training, the agent achieves significantly higher rewards (e.g., 101.0), meaning it has learned an effective strategy for balancing the pole.\n",
    "While this isn’t perfect performance (max reward = 500), it shows a marked improvement compared to the early episodes.\n",
    "Low Epsilon (0.08):\n",
    "The agent now primarily exploits its learned policy, relying on the Q-values to select the best action most of the time.\n",
    "Occasional exploration (8% of actions) helps the agent adapt to minor variations or avoid getting stuck in suboptimal policies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8a0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
