{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a426e024",
   "metadata": {},
   "source": [
    "#### Question 2: Mountain Car with Q-Learning\n",
    "#### Dataset Problem: Use OpenAI Gym's MountainCar-v0 environment to train a Q-learning agent.\n",
    "#### Similar to the CartPole example, but with the Mountain Car environment. The Q-learning code will be similar, with adjustments to the state and action space to fit the Mountain Car environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198ec465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make('MountainCar-v0')\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "episodes = 1000\n",
    "bins = (20, 20)  # Discretization bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2bc3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update env.reset() to unpack the state correctly\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Discretize the state function remains the same\n",
    "def discretize_state(state, env, bins):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    env_bins = [np.linspace(low, high, num=b) for low, high, b in zip(env_low, env_high, bins)]\n",
    "    return tuple(np.digitize(s, bins) for s, bins in zip(state, env_bins))\n",
    "\n",
    "# Use the updated state\n",
    "discretized_state = discretize_state(state, env, bins)\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    return np.argmax(q_table[state])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2c2daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 594/1000, Reward: -182.0\n",
      "Episode: 598/1000, Reward: -194.0\n",
      "Episode: 601/1000, Reward: -173.0\n",
      "Episode: 603/1000, Reward: -157.0\n",
      "Episode: 605/1000, Reward: -173.0\n",
      "Episode: 608/1000, Reward: -163.0\n",
      "Episode: 612/1000, Reward: -197.0\n",
      "Episode: 725/1000, Reward: -170.0\n",
      "Episode: 741/1000, Reward: -180.0\n",
      "Episode: 751/1000, Reward: -165.0\n",
      "Episode: 753/1000, Reward: -165.0\n",
      "Episode: 757/1000, Reward: -178.0\n",
      "Episode: 778/1000, Reward: -176.0\n",
      "Episode: 832/1000, Reward: -164.0\n",
      "Episode: 834/1000, Reward: -160.0\n",
      "Episode: 852/1000, Reward: -159.0\n",
      "Episode: 855/1000, Reward: -200.0\n",
      "Episode: 856/1000, Reward: -175.0\n",
      "Episode: 858/1000, Reward: -165.0\n",
      "Episode: 859/1000, Reward: -168.0\n",
      "Episode: 860/1000, Reward: -197.0\n",
      "Episode: 861/1000, Reward: -172.0\n",
      "Episode: 865/1000, Reward: -169.0\n",
      "Episode: 867/1000, Reward: -186.0\n",
      "Episode: 871/1000, Reward: -184.0\n",
      "Episode: 872/1000, Reward: -173.0\n",
      "Episode: 876/1000, Reward: -164.0\n",
      "Episode: 886/1000, Reward: -175.0\n",
      "Episode: 915/1000, Reward: -169.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()  # Correct handling of env.reset()\n",
    "    state = discretize_state(state, env, bins)\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(200):  # Max steps per episode\n",
    "        # Choose an action\n",
    "        action = epsilon_greedy(state, epsilon)\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, done, *info = env.step(action)  # Updated for new Gym API\n",
    "        next_state = discretize_state(next_state, env, bins)\n",
    "\n",
    "        # Q-value update\n",
    "        q_table[state][action] += learning_rate * (\n",
    "            reward + gamma * np.max(q_table[next_state]) - q_table[state][action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {episode+1}/{episodes}, Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "    # Update epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69f1ff",
   "metadata": {},
   "source": [
    "#### Exploration Dominates Early on:\n",
    "\n",
    "During the initial stages of training, the agent explores randomly. If epsilon (exploration rate) is still high, it may be choosing suboptimal actions and resulting in negative rewards.\n",
    "\n",
    "#### Inefficient Q-value Updates:\n",
    "\n",
    "Q-values might be updating too slowly, or there may not be enough learning from past experiences (due to poor exploration, too small learning rate, or issues with the reward structure).\n",
    "\n",
    "#### Exploration-Exploitation Balance:\n",
    "\n",
    "The agent may still be exploring too much, meaning it hasn't yet learned to exploit its knowledge effectively to maximize rewards.\n",
    "\n",
    "#### Poor Reward Function:\n",
    "\n",
    "The environment or the reward function might be insufficient for the agent to easily learn optimal behavior. The agent might be learning suboptimal strategies early on.\n",
    "\n",
    "#### Q-table/Network Initialization:\n",
    "\n",
    "If using Q-tables, the values are usually initialized arbitrarily. If the initialization is poor or if the learning rate is not appropriate, the Q-values could be slow to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa0b3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('MountainCar-v0')  # Change to 'CartPole-v1' if needed\n",
    "\n",
    "# Define hyperparameters\n",
    "learning_rate = 0.5  # Controls how much to adjust Q-values\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.05  # Minimum exploration rate\n",
    "epsilon_decay = 0.999  # Decay factor for epsilon\n",
    "episodes = 7000  # Number of training episodes\n",
    "max_timesteps = 200  # Maximum steps per episode\n",
    "\n",
    "# Discretize state space (for MountainCar, discretization is not needed with DQN)\n",
    "# Create Q-table (or neural network for DQN)\n",
    "state_space_size = (10, 10)  # Define discretization bins for each dimension\n",
    "action_space_size = env.action_space.n  # Number of possible actions\n",
    "q_table = np.zeros(state_space_size + (action_space_size,))  # For Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0271f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning functions\n",
    "def discretize_state(state, bins):\n",
    "    \"\"\"\n",
    "    Convert continuous state to discrete state by using predefined bins.\n",
    "    \"\"\"\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "    \n",
    "    # Create bins for each state dimension\n",
    "    state_bins = [np.linspace(low, high, num=bins[i] + 1)[1:-1] for i, (low, high) in enumerate(zip(env_low, env_high))]\n",
    "    \n",
    "    # Discretize each state dimension into its corresponding bin\n",
    "    return tuple(np.digitize(s, bins) for s, bins in zip(state, state_bins))\n",
    "\n",
    "def epsilon_greedy(state, epsilon):\n",
    "    \"\"\"\n",
    "    Select an action based on epsilon-greedy strategy.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(action_space_size)  # Exploration\n",
    "    else:\n",
    "        return np.argmax(q_table[state])  # Exploitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd642965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -200.0, Epsilon: 0.999\n",
      "Episode 100, Total Reward: -200.0, Epsilon: 0.904\n",
      "Episode 200, Total Reward: -200.0, Epsilon: 0.818\n",
      "Episode 300, Total Reward: -200.0, Epsilon: 0.740\n",
      "Episode 400, Total Reward: -200.0, Epsilon: 0.670\n",
      "Episode 500, Total Reward: -200.0, Epsilon: 0.606\n",
      "Episode 600, Total Reward: -200.0, Epsilon: 0.548\n",
      "Episode 700, Total Reward: -200.0, Epsilon: 0.496\n",
      "Episode 800, Total Reward: -200.0, Epsilon: 0.449\n",
      "Episode 900, Total Reward: -200.0, Epsilon: 0.406\n",
      "Episode 1000, Total Reward: -200.0, Epsilon: 0.367\n",
      "Episode 1100, Total Reward: -200.0, Epsilon: 0.332\n",
      "Episode 1200, Total Reward: -200.0, Epsilon: 0.301\n",
      "Episode 1300, Total Reward: -200.0, Epsilon: 0.272\n",
      "Episode 1400, Total Reward: -200.0, Epsilon: 0.246\n",
      "Episode 1500, Total Reward: -200.0, Epsilon: 0.223\n",
      "Episode 1600, Total Reward: -200.0, Epsilon: 0.202\n",
      "Episode 1700, Total Reward: -200.0, Epsilon: 0.182\n",
      "Episode 1800, Total Reward: -200.0, Epsilon: 0.165\n",
      "Episode 1900, Total Reward: -200.0, Epsilon: 0.149\n",
      "Episode 2000, Total Reward: -200.0, Epsilon: 0.135\n",
      "Episode 2100, Total Reward: -200.0, Epsilon: 0.122\n",
      "Episode: 2107/7000, Reward: -194.0, Epsilon: 0.122\n",
      "Episode 2200, Total Reward: -200.0, Epsilon: 0.111\n",
      "Episode 2300, Total Reward: -200.0, Epsilon: 0.100\n",
      "Episode 2400, Total Reward: -200.0, Epsilon: 0.091\n",
      "Episode 2500, Total Reward: -200.0, Epsilon: 0.082\n",
      "Episode 2600, Total Reward: -200.0, Epsilon: 0.074\n",
      "Episode: 2635/7000, Reward: -181.0, Epsilon: 0.072\n",
      "Episode: 2638/7000, Reward: -183.0, Epsilon: 0.071\n",
      "Episode: 2639/7000, Reward: -187.0, Epsilon: 0.071\n",
      "Episode 2700, Total Reward: -200.0, Epsilon: 0.067\n",
      "Episode: 2714/7000, Reward: -193.0, Epsilon: 0.066\n",
      "Episode: 2719/7000, Reward: -124.0, Epsilon: 0.066\n",
      "Episode: 2720/7000, Reward: -188.0, Epsilon: 0.066\n",
      "Episode: 2782/7000, Reward: -194.0, Epsilon: 0.062\n",
      "Episode 2800, Total Reward: -200.0, Epsilon: 0.061\n",
      "Episode 2900, Total Reward: -200.0, Epsilon: 0.055\n",
      "Episode: 2949/7000, Reward: -182.0, Epsilon: 0.052\n",
      "Episode: 2951/7000, Reward: -162.0, Epsilon: 0.052\n",
      "Episode: 2952/7000, Reward: -174.0, Epsilon: 0.052\n",
      "Episode: 2953/7000, Reward: -178.0, Epsilon: 0.052\n",
      "Episode: 2954/7000, Reward: -161.0, Epsilon: 0.052\n",
      "Episode 3000, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 3017/7000, Reward: -180.0, Epsilon: 0.050\n",
      "Episode: 3019/7000, Reward: -173.0, Epsilon: 0.050\n",
      "Episode: 3021/7000, Reward: -154.0, Epsilon: 0.050\n",
      "Episode: 3047/7000, Reward: -155.0, Epsilon: 0.050\n",
      "Episode: 3048/7000, Reward: -170.0, Epsilon: 0.050\n",
      "Episode: 3049/7000, Reward: -155.0, Epsilon: 0.050\n",
      "Episode: 3050/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode: 3051/7000, Reward: -156.0, Epsilon: 0.050\n",
      "Episode: 3052/7000, Reward: -162.0, Epsilon: 0.050\n",
      "Episode: 3053/7000, Reward: -189.0, Epsilon: 0.050\n",
      "Episode 3100, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 3191/7000, Reward: -175.0, Epsilon: 0.050\n",
      "Episode 3200, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 3205/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode 3300, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 3400, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 3500, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 3589/7000, Reward: -181.0, Epsilon: 0.050\n",
      "Episode: 3594/7000, Reward: -184.0, Epsilon: 0.050\n",
      "Episode 3600, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 3602/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 3633/7000, Reward: -197.0, Epsilon: 0.050\n",
      "Episode: 3685/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode 3700, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 3716/7000, Reward: -159.0, Epsilon: 0.050\n",
      "Episode: 3717/7000, Reward: -171.0, Epsilon: 0.050\n",
      "Episode: 3718/7000, Reward: -174.0, Epsilon: 0.050\n",
      "Episode: 3719/7000, Reward: -184.0, Epsilon: 0.050\n",
      "Episode: 3720/7000, Reward: -169.0, Epsilon: 0.050\n",
      "Episode: 3721/7000, Reward: -182.0, Epsilon: 0.050\n",
      "Episode: 3722/7000, Reward: -166.0, Epsilon: 0.050\n",
      "Episode: 3723/7000, Reward: -178.0, Epsilon: 0.050\n",
      "Episode: 3724/7000, Reward: -161.0, Epsilon: 0.050\n",
      "Episode: 3725/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 3733/7000, Reward: -160.0, Epsilon: 0.050\n",
      "Episode: 3734/7000, Reward: -197.0, Epsilon: 0.050\n",
      "Episode: 3738/7000, Reward: -180.0, Epsilon: 0.050\n",
      "Episode: 3740/7000, Reward: -186.0, Epsilon: 0.050\n",
      "Episode 3800, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 3900, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 4000, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4063/7000, Reward: -180.0, Epsilon: 0.050\n",
      "Episode: 4069/7000, Reward: -192.0, Epsilon: 0.050\n",
      "Episode: 4070/7000, Reward: -181.0, Epsilon: 0.050\n",
      "Episode: 4071/7000, Reward: -185.0, Epsilon: 0.050\n",
      "Episode: 4073/7000, Reward: -166.0, Epsilon: 0.050\n",
      "Episode: 4076/7000, Reward: -173.0, Epsilon: 0.050\n",
      "Episode: 4077/7000, Reward: -179.0, Epsilon: 0.050\n",
      "Episode 4100, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4120/7000, Reward: -171.0, Epsilon: 0.050\n",
      "Episode: 4135/7000, Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4161/7000, Reward: -189.0, Epsilon: 0.050\n",
      "Episode: 4171/7000, Reward: -174.0, Epsilon: 0.050\n",
      "Episode 4200, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4248/7000, Reward: -190.0, Epsilon: 0.050\n",
      "Episode: 4263/7000, Reward: -168.0, Epsilon: 0.050\n",
      "Episode: 4277/7000, Reward: -182.0, Epsilon: 0.050\n",
      "Episode: 4282/7000, Reward: -172.0, Epsilon: 0.050\n",
      "Episode 4300, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4304/7000, Reward: -175.0, Epsilon: 0.050\n",
      "Episode: 4307/7000, Reward: -188.0, Epsilon: 0.050\n",
      "Episode: 4325/7000, Reward: -168.0, Epsilon: 0.050\n",
      "Episode: 4333/7000, Reward: -187.0, Epsilon: 0.050\n",
      "Episode: 4334/7000, Reward: -161.0, Epsilon: 0.050\n",
      "Episode: 4335/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 4360/7000, Reward: -169.0, Epsilon: 0.050\n",
      "Episode: 4361/7000, Reward: -166.0, Epsilon: 0.050\n",
      "Episode: 4362/7000, Reward: -169.0, Epsilon: 0.050\n",
      "Episode: 4363/7000, Reward: -167.0, Epsilon: 0.050\n",
      "Episode: 4364/7000, Reward: -162.0, Epsilon: 0.050\n",
      "Episode: 4372/7000, Reward: -190.0, Epsilon: 0.050\n",
      "Episode: 4375/7000, Reward: -196.0, Epsilon: 0.050\n",
      "Episode: 4376/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode: 4377/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode: 4387/7000, Reward: -155.0, Epsilon: 0.050\n",
      "Episode: 4396/7000, Reward: -169.0, Epsilon: 0.050\n",
      "Episode 4400, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4460/7000, Reward: -154.0, Epsilon: 0.050\n",
      "Episode: 4465/7000, Reward: -180.0, Epsilon: 0.050\n",
      "Episode: 4466/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode: 4474/7000, Reward: -158.0, Epsilon: 0.050\n",
      "Episode: 4489/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode 4500, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4529/7000, Reward: -178.0, Epsilon: 0.050\n",
      "Episode: 4546/7000, Reward: -167.0, Epsilon: 0.050\n",
      "Episode 4600, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 4700, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4702/7000, Reward: -189.0, Epsilon: 0.050\n",
      "Episode: 4705/7000, Reward: -175.0, Epsilon: 0.050\n",
      "Episode: 4706/7000, Reward: -156.0, Epsilon: 0.050\n",
      "Episode: 4708/7000, Reward: -171.0, Epsilon: 0.050\n",
      "Episode: 4709/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode: 4727/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 4728/7000, Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4730/7000, Reward: -191.0, Epsilon: 0.050\n",
      "Episode: 4732/7000, Reward: -185.0, Epsilon: 0.050\n",
      "Episode: 4735/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 4738/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 4745/7000, Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4755/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 4756/7000, Reward: -185.0, Epsilon: 0.050\n",
      "Episode: 4757/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 4758/7000, Reward: -177.0, Epsilon: 0.050\n",
      "Episode: 4767/7000, Reward: -185.0, Epsilon: 0.050\n",
      "Episode: 4785/7000, Reward: -164.0, Epsilon: 0.050\n",
      "Episode: 4786/7000, Reward: -181.0, Epsilon: 0.050\n",
      "Episode: 4788/7000, Reward: -172.0, Epsilon: 0.050\n",
      "Episode: 4789/7000, Reward: -181.0, Epsilon: 0.050\n",
      "Episode: 4790/7000, Reward: -151.0, Epsilon: 0.050\n",
      "Episode: 4795/7000, Reward: -157.0, Epsilon: 0.050\n",
      "Episode: 4797/7000, Reward: -148.0, Epsilon: 0.050\n",
      "Episode: 4798/7000, Reward: -147.0, Epsilon: 0.050\n",
      "Episode 4800, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4815/7000, Reward: -156.0, Epsilon: 0.050\n",
      "Episode: 4816/7000, Reward: -184.0, Epsilon: 0.050\n",
      "Episode: 4818/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 4824/7000, Reward: -154.0, Epsilon: 0.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4825/7000, Reward: -163.0, Epsilon: 0.050\n",
      "Episode: 4829/7000, Reward: -186.0, Epsilon: 0.050\n",
      "Episode: 4831/7000, Reward: -187.0, Epsilon: 0.050\n",
      "Episode: 4832/7000, Reward: -162.0, Epsilon: 0.050\n",
      "Episode: 4860/7000, Reward: -189.0, Epsilon: 0.050\n",
      "Episode: 4863/7000, Reward: -154.0, Epsilon: 0.050\n",
      "Episode: 4865/7000, Reward: -162.0, Epsilon: 0.050\n",
      "Episode: 4877/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 4887/7000, Reward: -188.0, Epsilon: 0.050\n",
      "Episode: 4888/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode 4900, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 4909/7000, Reward: -170.0, Epsilon: 0.050\n",
      "Episode: 4928/7000, Reward: -115.0, Epsilon: 0.050\n",
      "Episode: 4929/7000, Reward: -122.0, Epsilon: 0.050\n",
      "Episode: 4930/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 4931/7000, Reward: -127.0, Epsilon: 0.050\n",
      "Episode: 4940/7000, Reward: -182.0, Epsilon: 0.050\n",
      "Episode: 4960/7000, Reward: -192.0, Epsilon: 0.050\n",
      "Episode 5000, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5057/7000, Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5068/7000, Reward: -192.0, Epsilon: 0.050\n",
      "Episode: 5071/7000, Reward: -179.0, Epsilon: 0.050\n",
      "Episode 5100, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 5200, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5261/7000, Reward: -191.0, Epsilon: 0.050\n",
      "Episode: 5262/7000, Reward: -188.0, Epsilon: 0.050\n",
      "Episode: 5267/7000, Reward: -178.0, Epsilon: 0.050\n",
      "Episode: 5268/7000, Reward: -158.0, Epsilon: 0.050\n",
      "Episode: 5270/7000, Reward: -171.0, Epsilon: 0.050\n",
      "Episode: 5271/7000, Reward: -149.0, Epsilon: 0.050\n",
      "Episode: 5272/7000, Reward: -188.0, Epsilon: 0.050\n",
      "Episode: 5273/7000, Reward: -154.0, Epsilon: 0.050\n",
      "Episode: 5274/7000, Reward: -153.0, Epsilon: 0.050\n",
      "Episode: 5283/7000, Reward: -174.0, Epsilon: 0.050\n",
      "Episode: 5292/7000, Reward: -191.0, Epsilon: 0.050\n",
      "Episode 5300, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5387/7000, Reward: -165.0, Epsilon: 0.050\n",
      "Episode: 5388/7000, Reward: -186.0, Epsilon: 0.050\n",
      "Episode: 5389/7000, Reward: -187.0, Epsilon: 0.050\n",
      "Episode 5400, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5424/7000, Reward: -185.0, Epsilon: 0.050\n",
      "Episode: 5441/7000, Reward: -197.0, Epsilon: 0.050\n",
      "Episode: 5442/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 5444/7000, Reward: -195.0, Epsilon: 0.050\n",
      "Episode: 5455/7000, Reward: -192.0, Epsilon: 0.050\n",
      "Episode: 5457/7000, Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5458/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode: 5461/7000, Reward: -134.0, Epsilon: 0.050\n",
      "Episode: 5498/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 5499/7000, Reward: -154.0, Epsilon: 0.050\n",
      "Episode: 5500/7000, Reward: -164.0, Epsilon: 0.050\n",
      "Episode: 5501/7000, Reward: -186.0, Epsilon: 0.050\n",
      "Episode 5500, Total Reward: -186.0, Epsilon: 0.050\n",
      "Episode: 5507/7000, Reward: -163.0, Epsilon: 0.050\n",
      "Episode: 5508/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode: 5511/7000, Reward: -152.0, Epsilon: 0.050\n",
      "Episode: 5512/7000, Reward: -184.0, Epsilon: 0.050\n",
      "Episode: 5551/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 5568/7000, Reward: -187.0, Epsilon: 0.050\n",
      "Episode 5600, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 5700, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5705/7000, Reward: -162.0, Epsilon: 0.050\n",
      "Episode: 5716/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 5717/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 5718/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 5721/7000, Reward: -168.0, Epsilon: 0.050\n",
      "Episode: 5723/7000, Reward: -192.0, Epsilon: 0.050\n",
      "Episode: 5724/7000, Reward: -171.0, Epsilon: 0.050\n",
      "Episode: 5726/7000, Reward: -187.0, Epsilon: 0.050\n",
      "Episode: 5738/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 5782/7000, Reward: -189.0, Epsilon: 0.050\n",
      "Episode: 5783/7000, Reward: -162.0, Epsilon: 0.050\n",
      "Episode 5800, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5858/7000, Reward: -177.0, Epsilon: 0.050\n",
      "Episode 5900, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 5971/7000, Reward: -174.0, Epsilon: 0.050\n",
      "Episode 6000, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 6039/7000, Reward: -191.0, Epsilon: 0.050\n",
      "Episode: 6040/7000, Reward: -171.0, Epsilon: 0.050\n",
      "Episode: 6043/7000, Reward: -186.0, Epsilon: 0.050\n",
      "Episode: 6061/7000, Reward: -165.0, Epsilon: 0.050\n",
      "Episode: 6088/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 6089/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode: 6090/7000, Reward: -188.0, Epsilon: 0.050\n",
      "Episode: 6091/7000, Reward: -186.0, Epsilon: 0.050\n",
      "Episode: 6092/7000, Reward: -147.0, Epsilon: 0.050\n",
      "Episode: 6101/7000, Reward: -190.0, Epsilon: 0.050\n",
      "Episode 6100, Total Reward: -190.0, Epsilon: 0.050\n",
      "Episode: 6106/7000, Reward: -182.0, Epsilon: 0.050\n",
      "Episode: 6126/7000, Reward: -191.0, Epsilon: 0.050\n",
      "Episode: 6131/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode 6200, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 6300, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 6369/7000, Reward: -189.0, Epsilon: 0.050\n",
      "Episode 6400, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 6500, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 6546/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 6549/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode 6600, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 6644/7000, Reward: -136.0, Epsilon: 0.050\n",
      "Episode: 6650/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode 6700, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 6800, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode 6900, Total Reward: -200.0, Epsilon: 0.050\n",
      "Episode: 6940/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 6948/7000, Reward: -192.0, Epsilon: 0.050\n",
      "Episode: 6949/7000, Reward: -193.0, Epsilon: 0.050\n",
      "Episode: 6950/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 6951/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode: 6952/7000, Reward: -196.0, Epsilon: 0.050\n",
      "Episode: 6953/7000, Reward: -196.0, Epsilon: 0.050\n",
      "Episode: 6954/7000, Reward: -194.0, Epsilon: 0.050\n",
      "Episode: 6959/7000, Reward: -198.0, Epsilon: 0.050\n",
      "Episode: 6974/7000, Reward: -199.0, Epsilon: 0.050\n",
      "Episode: 6975/7000, Reward: -160.0, Epsilon: 0.050\n",
      "Episode: 6976/7000, Reward: -168.0, Epsilon: 0.050\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()  # Reset the environment, get the initial state\n",
    "    state = discretize_state(state, bins=(10, 10))  # Discretize state space\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    for t in range(max_timesteps):  # For each time step\n",
    "        # Select action based on epsilon-greedy policy\n",
    "        action = epsilon_greedy(state, epsilon)\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, done, *info = env.step(action)\n",
    "\n",
    "        # Discretize the next state\n",
    "        next_state = discretize_state(next_state, bins=(10, 10))\n",
    "\n",
    "        # Q-value update (Q-learning update rule)\n",
    "        q_table[state][action] += learning_rate * (\n",
    "            reward + gamma * np.max(q_table[next_state]) - q_table[state][action]\n",
    "        )\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {episode+1}/{episodes}, Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "            break\n",
    "\n",
    "    # Decay epsilon (reduce exploration) only when epsilon is greater than epsilon_min\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "        # Make sure epsilon does not go below the minimum value\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "\n",
    "    # Optionally, print Q-values at certain intervals\n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
    "\n",
    "# Close the environment after training\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a544b",
   "metadata": {},
   "source": [
    "#### Key aspects of the performance and convergence of the reinforcement learning (RL) algorithm:\n",
    "\n",
    "#### Rewards Remaining Suboptimal: \n",
    "Across episodes, the total rewards are persistently negative, predominantly oscillating around -200. This suggests that the agent has difficulty learning an effective policy or that the environment and reward structure are particularly challenging.\n",
    "\n",
    "#### Exploration Decay (Epsilon): \n",
    "Epsilon, the exploration rate, decreases significantly during the initial episodes, reflecting an appropriate decay schedule. By episode 3000, it stabilizes at 0.05, indicating a shift toward exploitation (leveraging learned policies) over exploration.\n",
    "\n",
    "#### Occasional Reward Improvement: \n",
    "There are occasional episodes where rewards slightly improve (e.g., -181, -153, -115). These instances suggest that the agent occasionally discovers promising policies but fails to sustain or generalize them.\n",
    "\n",
    "#### Plateau After Epsilon Stabilization: \n",
    "After epsilon reaches its minimum value (0.05), the agent's performance plateaus. This could imply that the agent is stuck in a local minimum or has converged to a suboptimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3625853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
