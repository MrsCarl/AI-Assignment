{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ea04f2",
   "metadata": {},
   "source": [
    "#### Question: Generating Fashion Products with GAN\n",
    "#### Dataset Problem: Use the Fashion MNIST dataset to train a GAN for generating new fashion product images. \n",
    "#### The Fashion-MNIST dataset is proposed as a more challenging replacement dataset for the\n",
    "#### MNIST dataset. It is a dataset comprised of 60,000 small square 28 Ã…~ 28-pixel grayscale images of items of 10 types of clothing, such as shoes, t-shirts, dresses, and more. The mapping of all 0-9 integers to class labels are listed below.\n",
    "0: T-shirt/top\n",
    "1: Trouser\n",
    "2: Pullover\n",
    "3: Dress\n",
    "4: Coat\n",
    "5: Sandal\n",
    "6: Shirt\n",
    "7: Sneaker\n",
    "8: Bag\n",
    "9: Ankle boot\n",
    "##### Download data from: https://github.com/zalandoresearch/fashion-mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e6e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load Fashion MNIST\n",
    "(train_images1, _), (_, _) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize images to [0, 1] and reshape\n",
    "train_images2 = train_images1 / 255.0\n",
    "train_images = np.expand_dims(train_images2, axis=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35424db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "305147e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2dbe147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         ...,\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e52e3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    inputs = tf.keras.layers.Input(shape=(100,))\n",
    "    x = tf.keras.layers.Dense(7*7*256, use_bias=False)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Reshape((7, 7, 256))(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1108ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    inputs = tf.keras.layers.Input(shape=(28, 28, 1))\n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same')(inputs)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3c5e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc7c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3fef8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input, output_dir=\"generated_images\"):\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate predictions\n",
    "    predictions = model(test_input, training=False)\n",
    "    predictions = (predictions * 127.5 + 127.5).numpy()  # Rescale images to [0, 255]\n",
    "    \n",
    "    # Plot and save the images\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Save the figure\n",
    "    filename = os.path.join(output_dir, f\"image_at_epoch_{epoch:04d}.png\")\n",
    "    plt.savefig(filename)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved image for epoch {epoch} at {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8ee4da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 50\n",
    "NOISE_DIM = 100\n",
    "BATCH_SIZE = 256\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# Seed for consistent image generation\n",
    "seed = tf.random.normal([num_examples_to_generate, NOISE_DIM])\n",
    "\n",
    "# Training Step\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        \n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "# Full Training Loop\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "        print(f'Epoch {epoch+1}/{epochs} complete')\n",
    "        generate_and_save_images(generator, epoch + 1, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c33825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 complete\n",
      "Saved image for epoch 1 at generated_images\\image_at_epoch_0001.png\n",
      "Epoch 2/50 complete\n",
      "Saved image for epoch 2 at generated_images\\image_at_epoch_0002.png\n",
      "Epoch 3/50 complete\n",
      "Saved image for epoch 3 at generated_images\\image_at_epoch_0003.png\n",
      "Epoch 4/50 complete\n",
      "Saved image for epoch 4 at generated_images\\image_at_epoch_0004.png\n",
      "Epoch 5/50 complete\n",
      "Saved image for epoch 5 at generated_images\\image_at_epoch_0005.png\n",
      "Epoch 6/50 complete\n",
      "Saved image for epoch 6 at generated_images\\image_at_epoch_0006.png\n",
      "Epoch 7/50 complete\n",
      "Saved image for epoch 7 at generated_images\\image_at_epoch_0007.png\n",
      "Epoch 8/50 complete\n",
      "Saved image for epoch 8 at generated_images\\image_at_epoch_0008.png\n",
      "Epoch 9/50 complete\n",
      "Saved image for epoch 9 at generated_images\\image_at_epoch_0009.png\n",
      "Epoch 10/50 complete\n",
      "Saved image for epoch 10 at generated_images\\image_at_epoch_0010.png\n",
      "Epoch 11/50 complete\n",
      "Saved image for epoch 11 at generated_images\\image_at_epoch_0011.png\n",
      "Epoch 12/50 complete\n",
      "Saved image for epoch 12 at generated_images\\image_at_epoch_0012.png\n",
      "Epoch 13/50 complete\n",
      "Saved image for epoch 13 at generated_images\\image_at_epoch_0013.png\n",
      "Epoch 14/50 complete\n",
      "Saved image for epoch 14 at generated_images\\image_at_epoch_0014.png\n",
      "Epoch 15/50 complete\n",
      "Saved image for epoch 15 at generated_images\\image_at_epoch_0015.png\n",
      "Epoch 16/50 complete\n",
      "Saved image for epoch 16 at generated_images\\image_at_epoch_0016.png\n",
      "Epoch 17/50 complete\n",
      "Saved image for epoch 17 at generated_images\\image_at_epoch_0017.png\n",
      "Epoch 18/50 complete\n",
      "Saved image for epoch 18 at generated_images\\image_at_epoch_0018.png\n",
      "Epoch 19/50 complete\n",
      "Saved image for epoch 19 at generated_images\\image_at_epoch_0019.png\n",
      "Epoch 20/50 complete\n",
      "Saved image for epoch 20 at generated_images\\image_at_epoch_0020.png\n",
      "Epoch 21/50 complete\n",
      "Saved image for epoch 21 at generated_images\\image_at_epoch_0021.png\n",
      "Epoch 22/50 complete\n",
      "Saved image for epoch 22 at generated_images\\image_at_epoch_0022.png\n",
      "Epoch 23/50 complete\n",
      "Saved image for epoch 23 at generated_images\\image_at_epoch_0023.png\n",
      "Epoch 24/50 complete\n",
      "Saved image for epoch 24 at generated_images\\image_at_epoch_0024.png\n",
      "Epoch 25/50 complete\n",
      "Saved image for epoch 25 at generated_images\\image_at_epoch_0025.png\n",
      "Epoch 26/50 complete\n",
      "Saved image for epoch 26 at generated_images\\image_at_epoch_0026.png\n",
      "Epoch 27/50 complete\n",
      "Saved image for epoch 27 at generated_images\\image_at_epoch_0027.png\n",
      "Epoch 28/50 complete\n",
      "Saved image for epoch 28 at generated_images\\image_at_epoch_0028.png\n",
      "Epoch 29/50 complete\n",
      "Saved image for epoch 29 at generated_images\\image_at_epoch_0029.png\n",
      "Epoch 30/50 complete\n",
      "Saved image for epoch 30 at generated_images\\image_at_epoch_0030.png\n",
      "Epoch 31/50 complete\n",
      "Saved image for epoch 31 at generated_images\\image_at_epoch_0031.png\n",
      "Epoch 32/50 complete\n",
      "Saved image for epoch 32 at generated_images\\image_at_epoch_0032.png\n",
      "Epoch 33/50 complete\n",
      "Saved image for epoch 33 at generated_images\\image_at_epoch_0033.png\n",
      "Epoch 34/50 complete\n",
      "Saved image for epoch 34 at generated_images\\image_at_epoch_0034.png\n",
      "Epoch 35/50 complete\n",
      "Saved image for epoch 35 at generated_images\\image_at_epoch_0035.png\n",
      "Epoch 36/50 complete\n",
      "Saved image for epoch 36 at generated_images\\image_at_epoch_0036.png\n",
      "Epoch 37/50 complete\n",
      "Saved image for epoch 37 at generated_images\\image_at_epoch_0037.png\n",
      "Epoch 38/50 complete\n",
      "Saved image for epoch 38 at generated_images\\image_at_epoch_0038.png\n",
      "Epoch 39/50 complete\n",
      "Saved image for epoch 39 at generated_images\\image_at_epoch_0039.png\n",
      "Epoch 40/50 complete\n",
      "Saved image for epoch 40 at generated_images\\image_at_epoch_0040.png\n",
      "Epoch 41/50 complete\n",
      "Saved image for epoch 41 at generated_images\\image_at_epoch_0041.png\n",
      "Epoch 42/50 complete\n",
      "Saved image for epoch 42 at generated_images\\image_at_epoch_0042.png\n",
      "Epoch 43/50 complete\n",
      "Saved image for epoch 43 at generated_images\\image_at_epoch_0043.png\n",
      "Epoch 44/50 complete\n",
      "Saved image for epoch 44 at generated_images\\image_at_epoch_0044.png\n",
      "Epoch 45/50 complete\n",
      "Saved image for epoch 45 at generated_images\\image_at_epoch_0045.png\n",
      "Epoch 46/50 complete\n",
      "Saved image for epoch 46 at generated_images\\image_at_epoch_0046.png\n",
      "Epoch 47/50 complete\n",
      "Saved image for epoch 47 at generated_images\\image_at_epoch_0047.png\n",
      "Epoch 48/50 complete\n",
      "Saved image for epoch 48 at generated_images\\image_at_epoch_0048.png\n",
      "Epoch 49/50 complete\n",
      "Saved image for epoch 49 at generated_images\\image_at_epoch_0049.png\n",
      "Epoch 50/50 complete\n",
      "Saved image for epoch 50 at generated_images\\image_at_epoch_0050.png\n"
     ]
    }
   ],
   "source": [
    "# Prepare Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(60000).batch(BATCH_SIZE)\n",
    "\n",
    "# Train the GAN\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9df5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ac5e695",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) are a class of machine learning models designed to generate new data samples that resemble a given dataset. They consist of two neural networks, a Generator and a Discriminator, which are trained together in a competitive process.\n",
    "\n",
    "Key Components of GANs:\n",
    "Generator (G):\n",
    "\n",
    "The Generator creates new data samples.\n",
    "It starts with random noise (latent space input) and learns to transform it into data that resembles the training dataset.\n",
    "The Generator's goal is to fool the Discriminator into thinking the generated data is real.\n",
    "Discriminator (D):\n",
    "\n",
    "The Discriminator is a binary classifier.\n",
    "It takes both real data (from the dataset) and fake data (from the Generator) as input and attempts to classify them as real or fake.\n",
    "The Discriminatorâ€™s goal is to correctly distinguish between real and fake data.\n",
    "\n",
    "How GANs Work:\n",
    "The Generator creates a sample from random noise.\n",
    "The Discriminator evaluates this sample along with real samples.\n",
    "Both networks are updated:\n",
    "The Generator is updated to produce better \"fakes.\"\n",
    "The Discriminator is updated to better distinguish real from fake samples.\n",
    "This process continues until the Generator produces samples so realistic that the Discriminator can no longer reliably distinguish between real and fake.\n",
    "\n",
    "Understanding the Losses:\n",
    "Generator Loss: Indicates how well the generator is fooling the discriminator.\n",
    "Ideally, this should gradually decrease, showing that the generator is improving over time.\n",
    "Discriminator Loss: Indicates how well the discriminator distinguishes between real and generated data.\n",
    "This should ideally stay around 0.5 (indicating it cannot confidently distinguish between real and fake).\n",
    "If one model's loss dominates (e.g., discriminator loss close to 0, generator loss increasing sharply), training may be imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645a402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
